"""
ç°ä»£åŒ–èŠå¤©æœåŠ¡å±‚
åŒ…å«ç¼“å­˜ã€é‡è¯•æœºåˆ¶ã€æ€§èƒ½ç›‘æ§
"""
import asyncio
import json
import logging
import time
from typing import List, Optional, Dict, Any, AsyncGenerator
from dataclasses import dataclass
from enum import Enum
import openai
from tenacity import retry, stop_after_attempt, wait_exponential
from sqlalchemy.orm import Session

from backend.core.config import settings
from backend.models import User, ChatHistory

logger = logging.getLogger(__name__)

class ChatModel(str, Enum):
    """èŠå¤©æ¨¡å‹æšä¸¾"""
    DEEPSEEK_CHAT = "deepseek-chat"
    DEEPSEEK_REASONER = "deepseek-reasoner"

@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯æ•°æ®ç±»"""
    role: str
    content: str
    timestamp: Optional[float] = None
    metadata: Optional[Dict[str, Any]] = None

@dataclass 
class ChatRequest:
    """èŠå¤©è¯·æ±‚æ•°æ®ç±»"""
    message: str
    history: List[ChatMessage]
    model: ChatModel = ChatModel.DEEPSEEK_CHAT
    use_knowledge_base: bool = False
    stream: bool = True
    temperature: Optional[float] = None
    max_tokens: int = 4000

@dataclass
class ChatResponse:
    """èŠå¤©å“åº”æ•°æ®ç±»"""
    content: str
    reasoning: Optional[str] = None
    model: str = ""
    usage: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None

class ChatService:
    """èŠå¤©æœåŠ¡ç±»"""
    
    def __init__(self):
        self.client = openai.OpenAI(
            api_key=settings.deepseek_api_key,
            base_url=settings.deepseek_base_url,
            timeout=settings.deepseek_timeout
        )
        self._request_cache = {}
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        
    def _get_system_prompt(self, model: ChatModel, use_knowledge_base: bool = False) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        base_prompt = {
            ChatModel.DEEPSEEK_CHAT: """ä½ æ˜¯WePlusæ ¡å›­æ™ºèƒ½AIåŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºä¸­å›½æµ·æ´‹å¤§å­¦çš„å­¦ç”Ÿæä¾›æœåŠ¡ã€‚
ä½ å¯ä»¥å¸®åŠ©å­¦ç”Ÿè§£ç­”å…³äºæ ¡å›­ç”Ÿæ´»ã€å­¦ä¹ ã€æœåŠ¡ç­‰å„ç§é—®é¢˜ã€‚
è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­è°ƒå›ç­”é—®é¢˜ï¼Œå¹¶å°½é‡æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯ã€‚""",
            
            ChatModel.DEEPSEEK_REASONER: """ä½ æ˜¯WePlusæ ¡å›­æ™ºèƒ½AIåŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºä¸­å›½æµ·æ´‹å¤§å­¦çš„å­¦ç”Ÿæä¾›æœåŠ¡ã€‚
ä½ æ˜¯ä¸€ä¸ªå…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„AIåŠ©æ‰‹ï¼Œä¼šé€šè¿‡æ·±åº¦æ€è€ƒæ¥æä¾›å‡†ç¡®ç­”æ¡ˆã€‚
è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­è°ƒå›ç­”é—®é¢˜ï¼Œå¹¶å°½é‡æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯ã€‚"""
        }
        
        prompt = base_prompt.get(model, base_prompt[ChatModel.DEEPSEEK_CHAT])
        
        if use_knowledge_base:
            prompt += "\n\nä½ å¯ä»¥åŸºäºä¸Šä¼ çš„çŸ¥è¯†åº“æ–‡æ¡£æ¥å›ç­”æ›´ä¸“ä¸šçš„é—®é¢˜ã€‚"
            
        return prompt
    
    def _build_messages(self, request: ChatRequest) -> List[Dict[str, str]]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        system_prompt = self._get_system_prompt(request.model, request.use_knowledge_base)
        messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ å†å²æ¶ˆæ¯
        for msg in request.history:
            messages.append({"role": msg.role, "content": msg.content})
        
        # æ·»åŠ å½“å‰æ¶ˆæ¯
        messages.append({"role": "user", "content": request.message})
        
        return messages
    
    def _get_cache_key(self, messages: List[Dict[str, str]], model: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = json.dumps(messages, sort_keys=True) + model
        import hashlib
        return hashlib.md5(content.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        return time.time() - cache_entry["timestamp"] < self._cache_ttl
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def _call_api(self, messages: List[Dict[str, str]], model: ChatModel, stream: bool = False, **kwargs) -> Any:
        """è°ƒç”¨DeepSeek APIï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        api_params = {
            "model": model.value,
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", 4000),
            "stream": stream
        }
        
        # åªæœ‰é€šç”¨æ¨¡å‹æ”¯æŒæ¸©åº¦å‚æ•°
        if model == ChatModel.DEEPSEEK_CHAT:
            api_params["temperature"] = kwargs.get("temperature", 1.0)
        
        logger.info(f"ğŸ¤– è°ƒç”¨DeepSeek API: {model.value}, stream={stream}")
        
        try:
            response = self.client.chat.completions.create(**api_params)
            return response
        except Exception as e:
            logger.error(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """éæµå¼èŠå¤©"""
        messages = self._build_messages(request)
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(messages, request.model.value)
        if cache_key in self._request_cache:
            cache_entry = self._request_cache[cache_key]
            if self._is_cache_valid(cache_entry):
                logger.info(f"ğŸ’¾ èŠå¤©ç¼“å­˜å‘½ä¸­: {cache_key[:8]}...")
                return cache_entry["response"]
        
        # è°ƒç”¨API
        response = await self._call_api(
            messages, 
            request.model, 
            stream=False,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        # è§£æå“åº”
        chat_response = ChatResponse(
            content=response.choices[0].message.content,
            model=request.model.value,
            usage=response.usage.model_dump() if response.usage else None
        )
        
        # å¤„ç†æ¨ç†æ¨¡å‹çš„æ¨ç†å†…å®¹
        if (hasattr(response.choices[0].message, 'reasoning_content') and 
            response.choices[0].message.reasoning_content):
            chat_response.reasoning = response.choices[0].message.reasoning_content
        
        # ç¼“å­˜å“åº”
        self._request_cache[cache_key] = {
            "response": chat_response,
            "timestamp": time.time()
        }
        
        return chat_response
    
    async def chat_stream(self, request: ChatRequest) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼èŠå¤©"""
        messages = self._build_messages(request)
        
        # è°ƒç”¨API
        response = await self._call_api(
            messages, 
            request.model, 
            stream=True,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        assistant_response = ""
        reasoning_content = ""
        
        try:
            for chunk in response:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    
                    # å¤„ç†æ¨ç†å†…å®¹
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        reasoning = delta.reasoning_content
                        reasoning_content += reasoning
                        yield {
                            "type": "reasoning",
                            "data": reasoning,
                            "model": request.model.value
                        }
                    
                    # å¤„ç†æ™®é€šå†…å®¹
                    if hasattr(delta, 'content') and delta.content:
                        content = delta.content
                        assistant_response += content
                        yield {
                            "type": "content",
                            "data": content,
                            "model": request.model.value
                        }
            
            # å‘é€å®Œæˆä¿¡å·
            yield {
                "type": "done",
                "model": request.model.value,
                "total_content": assistant_response,
                "reasoning": reasoning_content if reasoning_content else None
            }
            
        except Exception as e:
            logger.error(f"âŒ æµå¼èŠå¤©å¤„ç†é”™è¯¯: {e}")
            yield {
                "type": "error",
                "data": f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "model": request.model.value
            }
    
    async def save_chat_history(self, db: Session, user_id: int, message: str, response: str, model: str = ""):
        """ä¿å­˜èŠå¤©å†å²"""
        try:
            chat_record = ChatHistory(
                user_id=user_id,
                message=message,
                response=response,
                model=model if hasattr(ChatHistory, 'model') else None
            )
            db.add(chat_record)
            db.commit()
            logger.info(f"ğŸ’¾ èŠå¤©å†å²ä¿å­˜æˆåŠŸ: user_id={user_id}")
        except Exception as e:
            logger.error(f"âŒ èŠå¤©å†å²ä¿å­˜å¤±è´¥: {e}")
            db.rollback()
            raise
    
    async def get_chat_history(self, db: Session, user_id: int, limit: int = 50) -> List[ChatHistory]:
        """è·å–èŠå¤©å†å²"""
        try:
            history = db.query(ChatHistory)\
                .filter(ChatHistory.user_id == user_id)\
                .order_by(ChatHistory.created_at.desc())\
                .limit(limit)\
                .all()
            
            return list(reversed(history))  # æŒ‰æ—¶é—´æ­£åºè¿”å›
        except Exception as e:
            logger.error(f"âŒ è·å–èŠå¤©å†å²å¤±è´¥: {e}")
            return []
    
    async def clear_chat_history(self, db: Session, user_id: int):
        """æ¸…ç©ºèŠå¤©å†å²"""
        try:
            db.query(ChatHistory)\
                .filter(ChatHistory.user_id == user_id)\
                .delete()
            db.commit()
            logger.info(f"ğŸ—‘ï¸ èŠå¤©å†å²æ¸…ç©ºæˆåŠŸ: user_id={user_id}")
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºèŠå¤©å†å²å¤±è´¥: {e}")
            db.rollback()
            raise
    
    def get_metrics(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡æŒ‡æ ‡"""
        return {
            "cache_size": len(self._request_cache),
            "cache_hit_rate": 0.0,  # å¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„ç»Ÿè®¡
            "api_timeout": settings.deepseek_timeout,
            "model_support": [model.value for model in ChatModel]
        }
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self._request_cache.clear()
        logger.info("ğŸ§¹ èŠå¤©æœåŠ¡ç¼“å­˜å·²æ¸…ç†")

# å…¨å±€èŠå¤©æœåŠ¡å®ä¾‹
chat_service = ChatService()

def get_chat_service() -> ChatService:
    """è·å–èŠå¤©æœåŠ¡å®ä¾‹"""
    return chat_service 